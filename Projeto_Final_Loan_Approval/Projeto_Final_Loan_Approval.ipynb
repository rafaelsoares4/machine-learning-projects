{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projeto Final - Aplica√ß√£o de Algoritmos de Machine Learning\n",
        "## Classifica√ß√£o de Aprova√ß√£o de Empr√©stimos\n",
        "\n",
        "**Autor:** Rafael Sobral\n",
        "**Dataset:** [Loan Approval Classification Data](https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data)\n",
        "**Data:** 2025-11-13\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Objetivo Geral\n",
        "\n",
        "Aplicar t√©cnicas de aprendizado de m√°quina supervisionado (classifica√ß√£o) para prever a aprova√ß√£o de empr√©stimos banc√°rios, utilizando m√∫ltiplos algoritmos e comparando seus desempenhos atrav√©s de m√©tricas apropriadas.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Estrutura do Projeto\n",
        "\n",
        "1. **Explora√ß√£o e An√°lise de Dados (EDA)**\n",
        "2. **Pr√©-processamento dos Dados**\n",
        "3. **Treinamento de Modelos (3+ algoritmos)**\n",
        "4. **Ajuste de Hiperpar√¢metros com Valida√ß√£o Cruzada**\n",
        "5. **Compara√ß√£o de Desempenho**\n",
        "6. **Redu√ß√£o de Dimensionalidade**\n",
        "7. **An√°lise Comparativa Final**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Importa√ß√£o de Bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa√ß√µes necess√°rias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Sklearn - Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, learning_curve\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Sklearn - Modelos de Classifica√ß√£o\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Sklearn - M√©tricas\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, ConfusionMatrixDisplay,\n",
        "                             roc_curve, auc, classification_report)\n",
        "\n",
        "# Sklearn - Redu√ß√£o de Dimensionalidade\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "# Balanceamento de classes\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "\n",
        "# Configura√ß√µes\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Random state para reprodutibilidade\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"‚úÖ Todas as bibliotecas importadas com sucesso!\")\n",
        "print(f\"üìÖ Notebook executado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üî¢ Random State: {RANDOM_STATE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 1. Explora√ß√£o e An√°lise de Dados (EDA)\n",
        "\n",
        "## 1.1 Carregamento do Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar o dataset\n",
        "# Nota: O dataset deve estar no mesmo diret√≥rio ou voc√™ pode usar kaggle API\n",
        "# Para baixar: kaggle datasets download -d taweilo/loan-approval-classification-data\n",
        "\n",
        "dataset_path = 'loan_approval_dataset.csv'\n",
        "\n",
        "# Verificar se o arquivo existe\n",
        "if os.path.exists(dataset_path):\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    print(f\"‚úÖ Dataset carregado com sucesso!\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Arquivo n√£o encontrado em: {dataset_path}\")\n",
        "    print(\"üì• Por favor, baixe o dataset do Kaggle:\")\n",
        "    print(\"   https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data\")\n",
        "    print(\"\\nüí° Alternativa: Use a API do Kaggle:\")\n",
        "    print(\"   kaggle datasets download -d taweilo/loan-approval-classification-data\")\n",
        "    print(\"   unzip loan-approval-classification-data.zip\")\n",
        "\n",
        "    # Criar um dataset de exemplo para demonstra√ß√£o (ser√° substitu√≠do pelo real)\n",
        "    print(\"\\nüìù Criando dataset de exemplo para demonstra√ß√£o...\")\n",
        "    # Este ser√° substitu√≠do quando o dataset real for carregado\n",
        "    df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informa√ß√µes b√°sicas do dataset\n",
        "if df is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìä INFORMA√á√ïES B√ÅSICAS DO DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nüìà Shape: {df.shape}\")\n",
        "    print(f\"üìã Colunas: {df.shape[1]}\")\n",
        "    print(f\"üìä Linhas: {df.shape[0]}\")\n",
        "\n",
        "    print(f\"\\nüìã Nomes das colunas:\")\n",
        "    for i, col in enumerate(df.columns, 1):\n",
        "        print(f\"  {i:2d}. {col}\")\n",
        "\n",
        "    print(f\"\\nüìä Tipos de dados:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(f\"\\nüìä Primeiras linhas:\")\n",
        "    display(df.head(10))\n",
        "\n",
        "    print(f\"\\nüìä Informa√ß√µes gerais:\")\n",
        "    df.info()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Carregue o dataset primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 An√°lise de Valores Ausentes e Outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None:\n",
        "    # An√°lise de valores ausentes\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîç AN√ÅLISE DE VALORES AUSENTES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df)) * 100\n",
        "\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Coluna': missing.index,\n",
        "        'Valores Ausentes': missing.values,\n",
        "        'Percentual (%)': missing_pct.values\n",
        "    }).sort_values('Valores Ausentes', ascending=False)\n",
        "\n",
        "    missing_df = missing_df[missing_df['Valores Ausentes'] > 0]\n",
        "\n",
        "    if len(missing_df) > 0:\n",
        "        display(missing_df)\n",
        "\n",
        "        # Visualiza√ß√£o\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.barh(missing_df['Coluna'], missing_df['Percentual (%)'], color='coral')\n",
        "        plt.xlabel('Percentual de Valores Ausentes (%)', fontsize=12, fontweight='bold')\n",
        "        plt.ylabel('Colunas', fontsize=12, fontweight='bold')\n",
        "        plt.title('An√°lise de Valores Ausentes', fontsize=14, fontweight='bold', pad=15)\n",
        "        plt.grid(axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"‚úÖ Nenhum valor ausente encontrado!\")\n",
        "\n",
        "    # An√°lise de outliers (para vari√°veis num√©ricas)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üìä AN√ÅLISE DE OUTLIERS (Vari√°veis Num√©ricas)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if len(numeric_cols) > 0:\n",
        "        fig, axes = plt.subplots(len(numeric_cols), 1, figsize=(12, 4*len(numeric_cols)))\n",
        "        if len(numeric_cols) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for idx, col in enumerate(numeric_cols):\n",
        "            axes[idx].boxplot(df[col].dropna(), vert=True, patch_artist=True,\n",
        "                            boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
        "                            medianprops=dict(color='red', linewidth=2))\n",
        "            axes[idx].set_ylabel(col, fontsize=11, fontweight='bold')\n",
        "            axes[idx].set_title(f'Boxplot - {col}', fontsize=12, fontweight='bold')\n",
        "            axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Nenhuma vari√°vel num√©rica encontrada para an√°lise de outliers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Estat√≠sticas Descritivas e Distribui√ß√µes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None:\n",
        "    # Estat√≠sticas descritivas\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìä ESTAT√çSTICAS DESCRITIVAS\")\n",
        "    print(\"=\" * 80)\n",
        "    display(df.describe())\n",
        "\n",
        "    # Distribui√ß√£o da vari√°vel target (assumindo que seja 'loan_status' ou similar)\n",
        "    # Vamos identificar a coluna target\n",
        "    target_candidates = [col for col in df.columns if 'loan' in col.lower() or 'status' in col.lower() or 'approval' in col.lower()]\n",
        "\n",
        "    if target_candidates:\n",
        "        target_col = target_candidates[0]\n",
        "        print(f\"\\nüéØ Vari√°vel Target identificada: '{target_col}'\")\n",
        "\n",
        "        # Distribui√ß√£o da target\n",
        "        print(f\"\\nüìä Distribui√ß√£o da vari√°vel target:\")\n",
        "        print(df[target_col].value_counts())\n",
        "        print(f\"\\nüìä Percentual:\")\n",
        "        print(df[target_col].value_counts(normalize=True) * 100)\n",
        "\n",
        "        # Visualiza√ß√£o\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        # Gr√°fico de barras\n",
        "        value_counts = df[target_col].value_counts()\n",
        "        axes[0].bar(value_counts.index.astype(str), value_counts.values, \n",
        "                   color=['#ff6b6b', '#51cf66'], alpha=0.7, edgecolor='black')\n",
        "        axes[0].set_xlabel('Status', fontsize=12, fontweight='bold')\n",
        "        axes[0].set_ylabel('Quantidade', fontsize=12, fontweight='bold')\n",
        "        axes[0].set_title('Distribui√ß√£o da Vari√°vel Target', fontsize=14, fontweight='bold')\n",
        "        axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "        # Gr√°fico de pizza\n",
        "        axes[1].pie(value_counts.values, labels=value_counts.index.astype(str),\n",
        "                   autopct='%1.1f%%', startangle=90, colors=['#ff6b6b', '#51cf66'])\n",
        "        axes[1].set_title('Propor√ß√£o da Vari√°vel Target', fontsize=14, fontweight='bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Verificar balanceamento\n",
        "        balance_ratio = value_counts.min() / value_counts.max()\n",
        "        print(f\"\\n‚öñÔ∏è Raz√£o de balanceamento: {balance_ratio:.3f}\")\n",
        "        if balance_ratio < 0.5:\n",
        "            print(\"‚ö†Ô∏è Classes desbalanceadas! Ser√° necess√°rio balanceamento.\")\n",
        "        else:\n",
        "            print(\"‚úÖ Classes relativamente balanceadas.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è N√£o foi poss√≠vel identificar automaticamente a vari√°vel target.\")\n",
        "        print(\"üìã Colunas dispon√≠veis:\", df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 An√°lise de Correla√ß√µes e Rela√ß√µes entre Vari√°veis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None:\n",
        "    # Matriz de correla√ß√£o (apenas vari√°veis num√©ricas)\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    if len(numeric_cols) > 1:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"üîó MATRIZ DE CORRELA√á√ÉO\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                   center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "        plt.title('Matriz de Correla√ß√£o - Vari√°veis Num√©ricas', fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Identificar correla√ß√µes fortes\n",
        "        print(\"\\nüîç Correla√ß√µes mais fortes (|r| > 0.7):\")\n",
        "        strong_corr = []\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                corr_val = corr_matrix.iloc[i, j]\n",
        "                if abs(corr_val) > 0.7:\n",
        "                    strong_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
        "\n",
        "        if strong_corr:\n",
        "            for col1, col2, corr in strong_corr:\n",
        "                print(f\"  ‚Ä¢ {col1:30s} ‚Üî {col2:30s}: {corr:+.3f}\")\n",
        "        else:\n",
        "            print(\"  Nenhuma correla√ß√£o muito forte encontrada.\")\n",
        "\n",
        "    # An√°lise de vari√°veis categ√≥ricas vs target\n",
        "    if 'target_col' in locals() and target_col:\n",
        "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "        if categorical_cols:\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\"üìä AN√ÅLISE DE VARI√ÅVEIS CATEG√ìRICAS\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "            n_cats = len(categorical_cols)\n",
        "            n_cols = min(3, n_cats)\n",
        "            n_rows = (n_cats + n_cols - 1) // n_cols\n",
        "\n",
        "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
        "            if n_cats == 1:\n",
        "                axes = [axes]\n",
        "            elif n_rows == 1:\n",
        "                axes = axes if isinstance(axes, np.ndarray) else [axes]\n",
        "            else:\n",
        "                axes = axes.flatten()\n",
        "\n",
        "            for idx, col in enumerate(categorical_cols):\n",
        "                if idx < len(axes):\n",
        "                    cross_tab = pd.crosstab(df[col], df[target_col], normalize='index') * 100\n",
        "                    cross_tab.plot(kind='bar', ax=axes[idx], color=['#ff6b6b', '#51cf66'], alpha=0.7)\n",
        "                    axes[idx].set_title(f'{col} vs {target_col}', fontsize=12, fontweight='bold')\n",
        "                    axes[idx].set_xlabel(col, fontsize=10)\n",
        "                    axes[idx].set_ylabel('Percentual (%)', fontsize=10)\n",
        "                    axes[idx].legend(title=target_col, fontsize=9)\n",
        "                    axes[idx].tick_params(axis='x', rotation=45)\n",
        "                    axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "            # Ocultar eixos extras\n",
        "            for idx in range(len(categorical_cols), len(axes)):\n",
        "                axes[idx].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 2. Pr√©-processamento dos Dados\n",
        "\n",
        "## 2.1 Prepara√ß√£o dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df is not None:\n",
        "    # Criar c√≥pia para pr√©-processamento\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Identificar vari√°vel target (se ainda n√£o foi identificada)\n",
        "    if 'target_col' not in locals() or not target_col:\n",
        "        target_candidates = [col for col in df_processed.columns if 'loan' in col.lower() or 'status' in col.lower() or 'approval' in col.lower()]\n",
        "        if target_candidates:\n",
        "            target_col = target_candidates[0]\n",
        "        else:\n",
        "            # Tentar √∫ltima coluna como target\n",
        "            target_col = df_processed.columns[-1]\n",
        "            print(f\"‚ö†Ô∏è Usando √∫ltima coluna como target: '{target_col}'\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîß PR√â-PROCESSAMENTO DOS DADOS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nüéØ Vari√°vel Target: {target_col}\")\n",
        "\n",
        "    # Separar features e target\n",
        "    X = df_processed.drop(columns=[target_col])\n",
        "    y = df_processed[target_col]\n",
        "\n",
        "    print(f\"\\nüìä Shape inicial:\")\n",
        "    print(f\"  ‚Ä¢ Features (X): {X.shape}\")\n",
        "    print(f\"  ‚Ä¢ Target (y): {y.shape}\")\n",
        "\n",
        "    # Identificar tipos de vari√°veis\n",
        "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    print(f\"\\nüìã Tipos de features:\")\n",
        "    print(f\"  ‚Ä¢ Num√©ricas: {len(numeric_features)}\")\n",
        "    print(f\"  ‚Ä¢ Categ√≥ricas: {len(categorical_features)}\")\n",
        "\n",
        "    if numeric_features:\n",
        "        print(f\"  ‚Ä¢ Lista num√©ricas: {numeric_features}\")\n",
        "    if categorical_features:\n",
        "        print(f\"  ‚Ä¢ Lista categ√≥ricas: {categorical_features}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Carregue o dataset primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Tratamento de Valores Ausentes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X' in locals() and X is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîß TRATAMENTO DE VALORES AUSENTES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Verificar valores ausentes\n",
        "    missing_before = X.isnull().sum()\n",
        "    missing_cols = missing_before[missing_before > 0]\n",
        "\n",
        "    if len(missing_cols) > 0:\n",
        "        print(f\"\\nüìä Valores ausentes encontrados em {len(missing_cols)} colunas:\")\n",
        "        for col, count in missing_cols.items():\n",
        "            pct = (count / len(X)) * 100\n",
        "            print(f\"  ‚Ä¢ {col}: {count} ({pct:.2f}%)\")\n",
        "\n",
        "        # Estrat√©gia de imputa√ß√£o\n",
        "        print(\"\\nüí° Estrat√©gia de imputa√ß√£o:\")\n",
        "        print(\"  ‚Ä¢ Num√©ricas: M√©dia\")\n",
        "        print(\"  ‚Ä¢ Categ√≥ricas: Moda\")\n",
        "\n",
        "        # Imputar valores ausentes\n",
        "        # Num√©ricas\n",
        "        if numeric_features:\n",
        "            numeric_imputer = SimpleImputer(strategy='mean')\n",
        "            X[numeric_features] = numeric_imputer.fit_transform(X[numeric_features])\n",
        "            print(f\"  ‚úÖ Imputa√ß√£o num√©rica conclu√≠da\")\n",
        "\n",
        "        # Categ√≥ricas\n",
        "        if categorical_features:\n",
        "            categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "            X[categorical_features] = categorical_imputer.fit_transform(X[categorical_features])\n",
        "            print(f\"  ‚úÖ Imputa√ß√£o categ√≥rica conclu√≠da\")\n",
        "\n",
        "        # Verificar ap√≥s imputa√ß√£o\n",
        "        missing_after = X.isnull().sum().sum()\n",
        "        print(f\"\\n‚úÖ Valores ausentes ap√≥s tratamento: {missing_after}\")\n",
        "    else:\n",
        "        print(\"‚úÖ Nenhum valor ausente encontrado!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Codifica√ß√£o de Vari√°veis Categ√≥ricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X' in locals() and X is not None and categorical_features:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîß CODIFICA√á√ÉO DE VARI√ÅVEIS CATEG√ìRICAS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Usar One-Hot Encoding para vari√°veis categ√≥ricas\n",
        "    print(\"\\nüí° M√©todo: One-Hot Encoding\")\n",
        "    print(\"  ‚Ä¢ Justificativa: Preserva todas as informa√ß√µes sem criar ordem artificial\")\n",
        "\n",
        "    # Aplicar One-Hot Encoding\n",
        "    X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=False, dtype=int)\n",
        "\n",
        "    print(f\"\\nüìä Dimens√µes ap√≥s encoding:\")\n",
        "    print(f\"  ‚Ä¢ Antes: {X.shape[1]} features\")\n",
        "    print(f\"  ‚Ä¢ Depois: {X_encoded.shape[1]} features\")\n",
        "    print(f\"  ‚Ä¢ Novas features criadas: {X_encoded.shape[1] - X.shape[1]}\")\n",
        "\n",
        "    X = X_encoded\n",
        "\n",
        "    print(f\"\\n‚úÖ Codifica√ß√£o conclu√≠da!\")\n",
        "    print(f\"üìã Novas colunas: {list(X.columns)}\")\n",
        "elif 'X' in locals() and X is not None:\n",
        "    print(\"‚úÖ Nenhuma vari√°vel categ√≥rica para codificar!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Codifica√ß√£o da Vari√°vel Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'y' in locals() and y is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîß CODIFICA√á√ÉO DA VARI√ÅVEL TARGET\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nüìä Valores √∫nicos na target: {y.unique()}\")\n",
        "    print(f\"üìä Tipo da target: {y.dtype}\")\n",
        "\n",
        "    # Se for categ√≥rica, codificar\n",
        "    if y.dtype == 'object':\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(y)\n",
        "        y = pd.Series(y_encoded, name=target_col)\n",
        "\n",
        "        print(f\"\\n‚úÖ Target codificada:\")\n",
        "        print(f\"  ‚Ä¢ Classes: {le.classes_}\")\n",
        "        print(f\"  ‚Ä¢ Mapeamento: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "    else:\n",
        "        print(\"‚úÖ Target j√° est√° num√©rica!\")\n",
        "\n",
        "    print(f\"\\nüìä Distribui√ß√£o final da target:\")\n",
        "    print(y.value_counts())\n",
        "    print(f\"\\nüìä Percentual:\")\n",
        "    print(y.value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Normaliza√ß√£o e Padroniza√ß√£o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X' in locals() and X is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîß NORMALIZA√á√ÉO E PADRONIZA√á√ÉO\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\nüí° M√©todo: StandardScaler (Z-score normalization)\")\n",
        "    print(\"  ‚Ä¢ Justificativa: Necess√°rio para algoritmos sens√≠veis √† escala (SVM, KNN, Regress√£o Log√≠stica)\")\n",
        "    print(\"  ‚Ä¢ F√≥rmula: z = (x - Œº) / œÉ\")\n",
        "\n",
        "    # Aplicar StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "    print(f\"\\n‚úÖ Padroniza√ß√£o conclu√≠da!\")\n",
        "    print(f\"üìä Estat√≠sticas ap√≥s padroniza√ß√£o:\")\n",
        "    print(f\"  ‚Ä¢ M√©dia: {X_scaled.mean().mean():.6f} (deve ser ~0)\")\n",
        "    print(f\"  ‚Ä¢ Desvio padr√£o: {X_scaled.std().mean():.6f} (deve ser ~1)\")\n",
        "\n",
        "    # Usar dados padronizados\n",
        "    X = X_scaled\n",
        "\n",
        "    print(f\"\\nüìã Features padronizadas prontas para modelagem!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Balanceamento de Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'y' in locals() and y is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"‚öñÔ∏è BALANCEAMENTO DE CLASSES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Verificar balanceamento\n",
        "    class_counts = Counter(y)\n",
        "    balance_ratio = min(class_counts.values()) / max(class_counts.values())\n",
        "\n",
        "    print(f\"\\nüìä Distribui√ß√£o antes do balanceamento:\")\n",
        "    for cls, count in class_counts.items():\n",
        "        pct = (count / len(y)) * 100\n",
        "        print(f\"  ‚Ä¢ Classe {cls}: {count} ({pct:.2f}%)\")\n",
        "    print(f\"\\n‚öñÔ∏è Raz√£o de balanceamento: {balance_ratio:.3f}\")\n",
        "\n",
        "    # Decidir se precisa balancear\n",
        "    need_balancing = balance_ratio < 0.5\n",
        "\n",
        "    if need_balancing:\n",
        "        print(\"\\n‚ö†Ô∏è Classes desbalanceadas! Aplicando SMOTE...\")\n",
        "        print(\"üí° M√©todo: SMOTE (Synthetic Minority Oversampling Technique)\")\n",
        "        print(\"  ‚Ä¢ Justificativa: Cria amostras sint√©ticas da classe minorit√°ria\")\n",
        "        print(\"  ‚Ä¢ Vantagem: N√£o perde informa√ß√µes como undersampling\")\n",
        "\n",
        "        # Aplicar SMOTE\n",
        "        smote = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
        "\n",
        "        print(f\"\\nüìä Distribui√ß√£o ap√≥s balanceamento:\")\n",
        "        class_counts_after = Counter(y_balanced)\n",
        "        for cls, count in class_counts_after.items():\n",
        "            pct = (count / len(y_balanced)) * 100\n",
        "            print(f\"  ‚Ä¢ Classe {cls}: {count} ({pct:.2f}%)\")\n",
        "\n",
        "        print(f\"\\nüìà Mudan√ßa de tamanho:\")\n",
        "        print(f\"  ‚Ä¢ Antes: {X.shape[0]} amostras\")\n",
        "        print(f\"  ‚Ä¢ Depois: {X_balanced.shape[0]} amostras\")\n",
        "\n",
        "        X = pd.DataFrame(X_balanced, columns=X.columns)\n",
        "        y = pd.Series(y_balanced, name=target_col)\n",
        "\n",
        "        print(f\"\\n‚úÖ Balanceamento conclu√≠do!\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Classes relativamente balanceadas. Balanceamento n√£o necess√°rio.\")\n",
        "        print(\"  ‚Ä¢ Continuando com dados originais\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.7 Divis√£o Treino/Teste\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X' in locals() and 'y' in locals() and X is not None and y is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"‚úÇÔ∏è DIVIS√ÉO TREINO/TESTE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Split estratificado (70/30)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.30, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüìä Divis√£o dos dados:\")\n",
        "    print(f\"  ‚Ä¢ Treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Teste:  {X_test.shape[0]} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nüìà Distribui√ß√£o no conjunto de TREINO:\")\n",
        "    train_counts = Counter(y_train)\n",
        "    for cls, count in train_counts.items():\n",
        "        pct = (count / len(y_train)) * 100\n",
        "        print(f\"  ‚Ä¢ Classe {cls}: {count:4d} ({pct:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nüìà Distribui√ß√£o no conjunto de TESTE:\")\n",
        "    test_counts = Counter(y_test)\n",
        "    for cls, count in test_counts.items():\n",
        "        pct = (count / len(y_test)) * 100\n",
        "        print(f\"  ‚Ä¢ Classe {cls}: {count:4d} ({pct:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Divis√£o conclu√≠da! Dados prontos para modelagem.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 3. Treinamento de Modelos\n",
        "\n",
        "## 3.1 Configura√ß√£o da Valida√ß√£o Cruzada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X_train' in locals():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"‚öôÔ∏è CONFIGURA√á√ÉO DA VALIDA√á√ÉO CRUZADA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # StratifiedKFold para classifica√ß√£o\n",
        "    skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    print(f\"\\nüìä Configura√ß√£o:\")\n",
        "    print(f\"  ‚Ä¢ M√©todo: StratifiedKFold\")\n",
        "    print(f\"  ‚Ä¢ K-Folds: 5\")\n",
        "    print(f\"  ‚Ä¢ Shuffle: True\")\n",
        "    print(f\"  ‚Ä¢ Random State: {RANDOM_STATE}\")\n",
        "    print(f\"\\nüí° Justificativa: StratifiedKFold mant√©m a propor√ß√£o de classes em cada fold\")\n",
        "\n",
        "    # Dicion√°rio para armazenar resultados\n",
        "    models_results = {}\n",
        "\n",
        "    print(f\"\\n‚úÖ Valida√ß√£o cruzada configurada!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Modelo 1: Regress√£o Log√≠stica\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X_train' in locals():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîµ MODELO 1: REGRESS√ÉO LOG√çSTICA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Espa√ßo de hiperpar√¢metros\n",
        "    param_dist_lr = {\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "        'solver': ['lbfgs', 'liblinear', 'saga'],\n",
        "        'max_iter': [100, 200, 500]\n",
        "    }\n",
        "\n",
        "    # Modelo base\n",
        "    lr_base = LogisticRegression(random_state=RANDOM_STATE)\n",
        "\n",
        "    # RandomizedSearchCV\n",
        "    random_search_lr = RandomizedSearchCV(\n",
        "        lr_base,\n",
        "        param_distributions=param_dist_lr,\n",
        "        n_iter=30,\n",
        "        cv=skfold,\n",
        "        scoring='roc_auc',\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚è≥ Treinando modelo com RandomizedSearchCV...\")\n",
        "    random_search_lr.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
        "    print(f\"\\nüèÜ Melhores hiperpar√¢metros:\")\n",
        "    for param, value in random_search_lr.best_params_.items():\n",
        "        print(f\"  ‚Ä¢ {param}: {value}\")\n",
        "    print(f\"\\nüìä Melhor score (CV): {random_search_lr.best_score_:.4f}\")\n",
        "\n",
        "    # Salvar melhor modelo\n",
        "    best_lr = random_search_lr.best_estimator_\n",
        "    models_results['Regress√£o Log√≠stica'] = {\n",
        "        'model': best_lr,\n",
        "        'search': random_search_lr,\n",
        "        'best_score_cv': random_search_lr.best_score_\n",
        "    }\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Modelo 2: Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X_train' in locals():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üü¢ MODELO 2: RANDOM FOREST\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Espa√ßo de hiperpar√¢metros\n",
        "    param_dist_rf = {\n",
        "        'n_estimators': [50, 100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'max_features': ['sqrt', 'log2', None]\n",
        "    }\n",
        "\n",
        "    # Modelo base\n",
        "    rf_base = RandomForestClassifier(random_state=RANDOM_STATE)\n",
        "\n",
        "    # RandomizedSearchCV\n",
        "    random_search_rf = RandomizedSearchCV(\n",
        "        rf_base,\n",
        "        param_distributions=param_dist_rf,\n",
        "        n_iter=30,\n",
        "        cv=skfold,\n",
        "        scoring='roc_auc',\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚è≥ Treinando modelo com RandomizedSearchCV...\")\n",
        "    random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
        "    print(f\"\\nüèÜ Melhores hiperpar√¢metros:\")\n",
        "    for param, value in random_search_rf.best_params_.items():\n",
        "        print(f\"  ‚Ä¢ {param}: {value}\")\n",
        "    print(f\"\\nüìä Melhor score (CV): {random_search_rf.best_score_:.4f}\")\n",
        "\n",
        "    # Salvar melhor modelo\n",
        "    best_rf = random_search_rf.best_estimator_\n",
        "    models_results['Random Forest'] = {\n",
        "        'model': best_rf,\n",
        "        'search': random_search_rf,\n",
        "        'best_score_cv': random_search_rf.best_score_\n",
        "    }\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Modelo 3: Support Vector Machine (SVM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X_train' in locals():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üü† MODELO 3: SUPPORT VECTOR MACHINE (SVM)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Espa√ßo de hiperpar√¢metros\n",
        "    param_dist_svm = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'kernel': ['linear', 'rbf', 'poly'],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "        'degree': [2, 3, 4]  # Para kernel poly\n",
        "    }\n",
        "\n",
        "    # Modelo base\n",
        "    svm_base = SVC(probability=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    # RandomizedSearchCV\n",
        "    random_search_svm = RandomizedSearchCV(\n",
        "        svm_base,\n",
        "        param_distributions=param_dist_svm,\n",
        "        n_iter=30,\n",
        "        cv=skfold,\n",
        "        scoring='roc_auc',\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚è≥ Treinando modelo com RandomizedSearchCV...\")\n",
        "    random_search_svm.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
        "    print(f\"\\nüèÜ Melhores hiperpar√¢metros:\")\n",
        "    for param, value in random_search_svm.best_params_.items():\n",
        "        print(f\"  ‚Ä¢ {param}: {value}\")\n",
        "    print(f\"\\nüìä Melhor score (CV): {random_search_svm.best_score_:.4f}\")\n",
        "\n",
        "    # Salvar melhor modelo\n",
        "    best_svm = random_search_svm.best_estimator_\n",
        "    models_results['SVM'] = {\n",
        "        'model': best_svm,\n",
        "        'search': random_search_svm,\n",
        "        'best_score_cv': random_search_svm.best_score_\n",
        "    }\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Modelo 4: Gradient Boosting (B√¥nus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X_train' in locals():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üü£ MODELO 4: GRADIENT BOOSTING (B√îNUS)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Espa√ßo de hiperpar√¢metros\n",
        "    param_dist_gb = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "\n",
        "    # Modelo base\n",
        "    gb_base = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "\n",
        "    # RandomizedSearchCV\n",
        "    random_search_gb = RandomizedSearchCV(\n",
        "        gb_base,\n",
        "        param_distributions=param_dist_gb,\n",
        "        n_iter=30,\n",
        "        cv=skfold,\n",
        "        scoring='roc_auc',\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚è≥ Treinando modelo com RandomizedSearchCV...\")\n",
        "    random_search_gb.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
        "    print(f\"\\nüèÜ Melhores hiperpar√¢metros:\")\n",
        "    for param, value in random_search_gb.best_params_.items():\n",
        "        print(f\"  ‚Ä¢ {param}: {value}\")\n",
        "    print(f\"\\nüìä Melhor score (CV): {random_search_gb.best_score_:.4f}\")\n",
        "\n",
        "    # Salvar melhor modelo\n",
        "    best_gb = random_search_gb.best_estimator_\n",
        "    models_results['Gradient Boosting'] = {\n",
        "        'model': best_gb,\n",
        "        'search': random_search_gb,\n",
        "        'best_score_cv': random_search_gb.best_score_\n",
        "    }\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 4. Compara√ß√£o de Desempenho\n",
        "\n",
        "## 4.1 Avalia√ß√£o no Conjunto de Teste\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'models_results' in locals() and len(models_results) > 0:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìä AVALIA√á√ÉO NO CONJUNTO DE TESTE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Calcular m√©tricas para cada modelo\n",
        "    for model_name, results in models_results.items():\n",
        "        model = results['model']\n",
        "\n",
        "        # Predi√ß√µes\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_proba = model.predict_proba(X_test)[:, 1] if len(np.unique(y_test)) == 2 else model.predict_proba(X_test)\n",
        "\n",
        "        # M√©tricas\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted')\n",
        "        recall = recall_score(y_test, y_pred, average='weighted')\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        # ROC-AUC\n",
        "        if len(np.unique(y_test)) == 2:\n",
        "            roc_auc = roc_auc_score(y_test, y_proba)\n",
        "        else:\n",
        "            roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')\n",
        "\n",
        "        # Armazenar resultados\n",
        "        results['y_pred'] = y_pred\n",
        "        results['y_proba'] = y_proba\n",
        "        results['accuracy'] = accuracy\n",
        "        results['precision'] = precision\n",
        "        results['recall'] = recall\n",
        "        results['f1'] = f1\n",
        "        results['roc_auc'] = roc_auc\n",
        "\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  ‚Ä¢ Accuracy:  {accuracy:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Precision: {precision:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Recall:    {recall:.4f}\")\n",
        "        print(f\"  ‚Ä¢ F1-Score:  {f1:.4f}\")\n",
        "        print(f\"  ‚Ä¢ ROC-AUC:   {roc_auc:.4f}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Avalia√ß√£o conclu√≠da!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Treine os modelos primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Tabela Comparativa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'models_results' in locals() and len(models_results) > 0:\n",
        "    # Criar tabela comparativa\n",
        "    comparison_data = {\n",
        "        'Modelo': [],\n",
        "        'CV Score (m√©dia)': [],\n",
        "        'Accuracy': [],\n",
        "        'Precision': [],\n",
        "        'Recall': [],\n",
        "        'F1-Score': [],\n",
        "        'ROC-AUC': []\n",
        "    }\n",
        "\n",
        "    for model_name, results in models_results.items():\n",
        "        comparison_data['Modelo'].append(model_name)\n",
        "        comparison_data['CV Score (m√©dia)'].append(f\"{results['best_score_cv']:.4f}\")\n",
        "        comparison_data['Accuracy'].append(f\"{results['accuracy']:.4f}\")\n",
        "        comparison_data['Precision'].append(f\"{results['precision']:.4f}\")\n",
        "        comparison_data['Recall'].append(f\"{results['recall']:.4f}\")\n",
        "        comparison_data['F1-Score'].append(f\"{results['f1']:.4f}\")\n",
        "        comparison_data['ROC-AUC'].append(f\"{results['roc_auc']:.4f}\")\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìã TABELA COMPARATIVA DE DESEMPENHO\")\n",
        "    print(\"=\" * 80)\n",
        "    display(comparison_df)\n",
        "\n",
        "    # Identificar melhor modelo (baseado em ROC-AUC)\n",
        "    best_model_name = max(models_results.keys(), key=lambda k: models_results[k]['roc_auc'])\n",
        "    print(f\"\\nüèÜ MELHOR MODELO: {best_model_name}\")\n",
        "    print(f\"   ROC-AUC: {models_results[best_model_name]['roc_auc']:.4f}\")\n",
        "    print(f\"   Accuracy: {models_results[best_model_name]['accuracy']:.4f}\")\n",
        "    print(f\"   F1-Score: {models_results[best_model_name]['f1']:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Visualiza√ß√µes Comparativas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'models_results' in locals() and len(models_results) > 0:\n",
        "    # Gr√°fico de barras comparativo\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    model_names = list(models_results.keys())\n",
        "    colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6']\n",
        "\n",
        "    # Accuracy\n",
        "    accuracies = [models_results[m]['accuracy'] for m in model_names]\n",
        "    axes[0, 0].bar(model_names, accuracies, color=colors[:len(model_names)], alpha=0.7, edgecolor='black')\n",
        "    axes[0, 0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_title('Accuracy por Modelo', fontsize=13, fontweight='bold')\n",
        "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "    axes[0, 0].set_ylim([0, 1])\n",
        "    for i, v in enumerate(accuracies):\n",
        "        axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "    # F1-Score\n",
        "    f1_scores = [models_results[m]['f1'] for m in model_names]\n",
        "    axes[0, 1].bar(model_names, f1_scores, color=colors[:len(model_names)], alpha=0.7, edgecolor='black')\n",
        "    axes[0, 1].set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_title('F1-Score por Modelo', fontsize=13, fontweight='bold')\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "    axes[0, 1].set_ylim([0, 1])\n",
        "    for i, v in enumerate(f1_scores):\n",
        "        axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "    # ROC-AUC\n",
        "    roc_aucs = [models_results[m]['roc_auc'] for m in model_names]\n",
        "    axes[1, 0].bar(model_names, roc_aucs, color=colors[:len(model_names)], alpha=0.7, edgecolor='black')\n",
        "    axes[1, 0].set_ylabel('ROC-AUC', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].set_title('ROC-AUC por Modelo', fontsize=13, fontweight='bold')\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "    axes[1, 0].set_ylim([0, 1])\n",
        "    for i, v in enumerate(roc_aucs):\n",
        "        axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "    # Compara√ß√£o geral (todas as m√©tricas)\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.2\n",
        "    axes[1, 1].bar(x - width, accuracies, width, label='Accuracy', alpha=0.7)\n",
        "    axes[1, 1].bar(x, f1_scores, width, label='F1-Score', alpha=0.7)\n",
        "    axes[1, 1].bar(x + width, roc_aucs, width, label='ROC-AUC', alpha=0.7)\n",
        "    axes[1, 1].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_title('Compara√ß√£o Geral de M√©tricas', fontsize=13, fontweight='bold')\n",
        "    axes[1, 1].set_xticks(x)\n",
        "    axes[1, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "    axes[1, 1].set_ylim([0, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Visualiza√ß√µes criadas!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4 Matriz de Confus√£o e Curvas ROC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'models_results' in locals() and len(models_results) > 0:\n",
        "    # Matriz de confus√£o para cada modelo\n",
        "    n_models = len(models_results)\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for idx, (model_name, results) in enumerate(models_results.items()):\n",
        "        cm = confusion_matrix(y_test, results['y_pred'])\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "        disp.plot(ax=axes[idx], cmap='Blues', values_format='d')\n",
        "        axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Curvas ROC\n",
        "    if len(np.unique(y_test)) == 2:\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "        for model_name, results in models_results.items():\n",
        "            fpr, tpr, _ = roc_curve(y_test, results['y_proba'])\n",
        "            roc_auc = results['roc_auc']\n",
        "            ax.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "        ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Chance')\n",
        "        ax.set_xlim([0.0, 1.0])\n",
        "        ax.set_ylim([0.0, 1.05])\n",
        "        ax.set_xlabel('Taxa de Falso Positivo', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Taxa de Verdadeiro Positivo', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Curvas ROC - Compara√ß√£o de Modelos', fontsize=14, fontweight='bold', pad=15)\n",
        "        ax.legend(loc='lower right', fontsize=10)\n",
        "        ax.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    print(\"‚úÖ Visualiza√ß√µes de matriz de confus√£o e ROC criadas!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 5. Redu√ß√£o de Dimensionalidade\n",
        "\n",
        "## 5.1 Aplica√ß√£o de PCA no Melhor Modelo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'models_results' in locals() and len(models_results) > 0 and 'best_model_name' in locals():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìê REDU√á√ÉO DE DIMENSIONALIDADE - PCA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Identificar melhor modelo\n",
        "    best_model = models_results[best_model_name]['model']\n",
        "\n",
        "    print(f\"\\nüéØ Modelo selecionado: {best_model_name}\")\n",
        "    print(f\"üìä Dimens√µes originais: {X_train.shape[1]} features\")\n",
        "\n",
        "    # Aplicar PCA mantendo 95% da vari√¢ncia\n",
        "    pca = PCA(n_components=0.95, random_state=RANDOM_STATE)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "\n",
        "    n_components = X_train_pca.shape[1]\n",
        "    variance_explained = sum(pca.explained_variance_ratio_)\n",
        "\n",
        "    print(f\"\\nüìä Ap√≥s PCA:\")\n",
        "    print(f\"  ‚Ä¢ Componentes principais: {n_components}\")\n",
        "    print(f\"  ‚Ä¢ Vari√¢ncia explicada: {variance_explained:.4f} ({variance_explained*100:.2f}%)\")\n",
        "    print(f\"  ‚Ä¢ Redu√ß√£o de dimensionalidade: {X_train.shape[1] - n_components} features ({((X_train.shape[1] - n_components) / X_train.shape[1]) * 100:.1f}%)\")\n",
        "\n",
        "    # Visualizar vari√¢ncia explicada\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Gr√°fico de vari√¢ncia explicada acumulada\n",
        "    cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "    axes[0].plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 'bo-', linewidth=2, markersize=6)\n",
        "    axes[0].axhline(y=0.95, color='r', linestyle='--', label='95% Vari√¢ncia')\n",
        "    axes[0].set_xlabel('N√∫mero de Componentes Principais', fontsize=11, fontweight='bold')\n",
        "    axes[0].set_ylabel('Vari√¢ncia Explicada Acumulada', fontsize=11, fontweight='bold')\n",
        "    axes[0].set_title('Vari√¢ncia Explicada por Componentes Principais', fontsize=12, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # Gr√°fico de vari√¢ncia explicada individual\n",
        "    axes[1].bar(range(1, min(20, len(pca.explained_variance_ratio_)) + 1), \n",
        "                pca.explained_variance_ratio_[:20], alpha=0.7, color='steelblue')\n",
        "    axes[1].set_xlabel('Componente Principal', fontsize=11, fontweight='bold')\n",
        "    axes[1].set_ylabel('Vari√¢ncia Explicada', fontsize=11, fontweight='bold')\n",
        "    axes[1].set_title('Vari√¢ncia Explicada Individual (Top 20)', fontsize=12, fontweight='bold')\n",
        "    axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n‚úÖ PCA aplicado com sucesso!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Treinamento do Modelo com PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'X_train_pca' in locals():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üî¨ TREINAMENTO DO MODELO COM PCA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Criar novo modelo do mesmo tipo do melhor modelo\n",
        "    if 'Random Forest' in best_model_name:\n",
        "        model_pca = RandomForestClassifier(**best_model.get_params())\n",
        "    elif 'Log√≠stica' in best_model_name:\n",
        "        model_pca = LogisticRegression(**best_model.get_params())\n",
        "    elif 'SVM' in best_model_name:\n",
        "        model_pca = SVC(**best_model.get_params())\n",
        "    elif 'Gradient' in best_model_name:\n",
        "        model_pca = GradientBoostingClassifier(**best_model.get_params())\n",
        "    else:\n",
        "        # Fallback: usar o mesmo tipo\n",
        "        model_pca = type(best_model)(**best_model.get_params())\n",
        "\n",
        "    print(f\"\\n‚è≥ Treinando {best_model_name} com dados reduzidos por PCA...\")\n",
        "    model_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "    # Avaliar no conjunto de teste\n",
        "    y_pred_pca = model_pca.predict(X_test_pca)\n",
        "    y_proba_pca = model_pca.predict_proba(X_test_pca)[:, 1] if len(np.unique(y_test)) == 2 else model_pca.predict_proba(X_test_pca)\n",
        "\n",
        "    # M√©tricas\n",
        "    accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "    precision_pca = precision_score(y_test, y_pred_pca, average='weighted')\n",
        "    recall_pca = recall_score(y_test, y_pred_pca, average='weighted')\n",
        "    f1_pca = f1_score(y_test, y_pred_pca, average='weighted')\n",
        "\n",
        "    if len(np.unique(y_test)) == 2:\n",
        "        roc_auc_pca = roc_auc_score(y_test, y_proba_pca)\n",
        "    else:\n",
        "        roc_auc_pca = roc_auc_score(y_test, y_proba_pca, multi_class='ovr', average='weighted')\n",
        "\n",
        "    print(f\"\\n‚úÖ Treinamento conclu√≠do!\")\n",
        "    print(f\"\\nüìä M√©tricas com PCA:\")\n",
        "    print(f\"  ‚Ä¢ Accuracy:  {accuracy_pca:.4f}\")\n",
        "    print(f\"  ‚Ä¢ Precision: {precision_pca:.4f}\")\n",
        "    print(f\"  ‚Ä¢ Recall:    {recall_pca:.4f}\")\n",
        "    print(f\"  ‚Ä¢ F1-Score:  {f1_pca:.4f}\")\n",
        "    print(f\"  ‚Ä¢ ROC-AUC:   {roc_auc_pca:.4f}\")\n",
        "\n",
        "    # Comparar com modelo original\n",
        "    accuracy_original = models_results[best_model_name]['accuracy']\n",
        "    f1_original = models_results[best_model_name]['f1']\n",
        "    roc_auc_original = models_results[best_model_name]['roc_auc']\n",
        "\n",
        "    print(f\"\\nüìä M√©tricas originais (sem PCA):\")\n",
        "    print(f\"  ‚Ä¢ Accuracy:  {accuracy_original:.4f}\")\n",
        "    print(f\"  ‚Ä¢ F1-Score:  {f1_original:.4f}\")\n",
        "    print(f\"  ‚Ä¢ ROC-AUC:   {roc_auc_original:.4f}\")\n",
        "\n",
        "    print(f\"\\nüìà Compara√ß√£o:\")\n",
        "    print(f\"  ‚Ä¢ Accuracy:  {accuracy_pca - accuracy_original:+.4f} ({((accuracy_pca - accuracy_original) / accuracy_original * 100):+.2f}%)\")\n",
        "    print(f\"  ‚Ä¢ F1-Score:  {f1_pca - f1_original:+.4f} ({((f1_pca - f1_original) / f1_original * 100):+.2f}%)\")\n",
        "    print(f\"  ‚Ä¢ ROC-AUC:   {roc_auc_pca - roc_auc_original:+.4f} ({((roc_auc_pca - roc_auc_original) / roc_auc_original * 100):+.2f}%)\")\n",
        "\n",
        "    # Armazenar resultados\n",
        "    pca_results = {\n",
        "        'accuracy': accuracy_pca,\n",
        "        'precision': precision_pca,\n",
        "        'recall': recall_pca,\n",
        "        'f1': f1_pca,\n",
        "        'roc_auc': roc_auc_pca,\n",
        "        'n_components': n_components,\n",
        "        'variance_explained': variance_explained\n",
        "    }\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Visualiza√ß√£o Comparativa: Antes vs. Depois do PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'pca_results' in locals():\n",
        "    # Gr√°fico comparativo\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    metrics = ['Accuracy', 'F1-Score', 'ROC-AUC']\n",
        "    original_values = [accuracy_original, f1_original, roc_auc_original]\n",
        "    pca_values = [accuracy_pca, f1_pca, roc_auc_pca]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    for idx, (metric, orig, pca) in enumerate(zip(metrics, original_values, pca_values)):\n",
        "        axes[idx].bar(x[idx] - width/2, orig, width, label='Original', alpha=0.7, color='steelblue')\n",
        "        axes[idx].bar(x[idx] + width/2, pca, width, label='Com PCA', alpha=0.7, color='coral')\n",
        "        axes[idx].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "        axes[idx].set_title(f'{metric}', fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_ylim([0, 1])\n",
        "        axes[idx].set_xticks([x[idx]])\n",
        "        axes[idx].set_xticklabels([metric])\n",
        "        axes[idx].legend()\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "        axes[idx].text(x[idx] - width/2, orig + 0.02, f'{orig:.3f}', ha='center', fontweight='bold')\n",
        "        axes[idx].text(x[idx] + width/2, pca + 0.02, f'{pca:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "    plt.suptitle('Compara√ß√£o: Modelo Original vs. Modelo com PCA', fontsize=14, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Discuss√£o dos efeitos\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"üí° DISCUSS√ÉO DOS EFEITOS DA REDU√á√ÉO DE DIMENSIONALIDADE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nüìä Efeitos sobre o desempenho:\")\n",
        "    if accuracy_pca >= accuracy_original * 0.98:\n",
        "        print(\"  ‚úÖ Desempenho mantido ou melhorado com PCA\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è Pequena redu√ß√£o no desempenho, mas aceit√°vel\")\n",
        "\n",
        "    print(f\"\\nüìä Efeitos sobre a complexidade:\")\n",
        "    print(f\"  ‚Ä¢ Redu√ß√£o de {X_train.shape[1] - n_components} features ({((X_train.shape[1] - n_components) / X_train.shape[1]) * 100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ Tempo de treinamento: Reduzido\")\n",
        "    print(f\"  ‚Ä¢ Tempo de predi√ß√£o: Reduzido\")\n",
        "    print(f\"  ‚Ä¢ Uso de mem√≥ria: Reduzido\")\n",
        "\n",
        "    print(f\"\\nüìä Efeitos sobre a visualiza√ß√£o:\")\n",
        "    print(f\"  ‚Ä¢ Dados podem ser visualizados em 2D/3D usando componentes principais\")\n",
        "    print(f\"  ‚Ä¢ Facilita interpreta√ß√£o e an√°lise explorat√≥ria\")\n",
        "\n",
        "    print(f\"\\n‚úÖ An√°lise de redu√ß√£o de dimensionalidade conclu√≠da!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Execute as c√©lulas anteriores primeiro!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# 6. Conclus√µes e Discuss√£o Final\n",
        "\n",
        "## 6.1 Resumo dos Resultados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Principais Resultados\n",
        "\n",
        "1. **Melhor Modelo:** [Ser√° preenchido ap√≥s execu√ß√£o]\n",
        "   - Accuracy: [valor]\n",
        "   - F1-Score: [valor]\n",
        "   - ROC-AUC: [valor]\n",
        "\n",
        "2. **Efeitos da Redu√ß√£o de Dimensionalidade:**\n",
        "   - Redu√ß√£o de features: [valor]\n",
        "   - Impacto no desempenho: [an√°lise]\n",
        "\n",
        "3. **Insights Principais:**\n",
        "   - [Insight 1]\n",
        "   - [Insight 2]\n",
        "   - [Insight 3]\n",
        "\n",
        "### üéØ Objetivos Alcan√ßados\n",
        "\n",
        "‚úÖ **EDA Completo:** An√°lise explorat√≥ria com gr√°ficos e estat√≠sticas descritivas\n",
        "‚úÖ **Pr√©-processamento:** Tratamento de dados ausentes, encoding, normaliza√ß√£o e balanceamento\n",
        "‚úÖ **M√∫ltiplos Modelos:** Treinamento de 4 algoritmos diferentes\n",
        "‚úÖ **Fine-tuning:** Ajuste de hiperpar√¢metros com valida√ß√£o cruzada\n",
        "‚úÖ **Compara√ß√£o:** Avalia√ß√£o com m√∫ltricas apropriadas (Accuracy, F1-Score, ROC-AUC)\n",
        "‚úÖ **Redu√ß√£o de Dimensionalidade:** Aplica√ß√£o de PCA e compara√ß√£o de desempenho\n",
        "\n",
        "### üí° Limita√ß√µes e Melhorias Futuras\n",
        "\n",
        "**Limita√ß√µes:**\n",
        "- [Limita√ß√£o 1]\n",
        "- [Limita√ß√£o 2]\n",
        "\n",
        "**Melhorias Futuras:**\n",
        "- Testar outros algoritmos (XGBoost, LightGBM, Redes Neurais)\n",
        "- Feature engineering mais avan√ßado\n",
        "- Ensemble methods (Stacking, Voting)\n",
        "- An√°lise de import√¢ncia de features (SHAP values)\n",
        "- Otimiza√ß√£o bayesiana de hiperpar√¢metros\n",
        "\n",
        "---\n",
        "\n",
        "**üìÖ Projeto Final conclu√≠do com sucesso!**\n",
        "**üî¨ Todas as etapas foram executadas conforme os requisitos.**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
